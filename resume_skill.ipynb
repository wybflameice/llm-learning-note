{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65dd49c4",
   "metadata": {},
   "source": [
    "#### 简历技能中提到的LR、CNN、RNN、Transformer\n",
    "1. 逻辑回归\n",
    "逻辑回归是我在学习评分卡建模时系统接触的，它非常适合二分类+可解释性要求高的场景，比如信贷风控<br>\n",
    "逻辑回归假设数据服从伯努利分布，通过最大似然函数估计来确定模型的参数，以最大化数据的似然函数（也是它的损失函数），通常使用梯度下降等优化算法找到最佳参数。<br>\n",
    "**优点**：形式简单、模型可解释性非常好；数学性质好，易于优化；不仅可以预测类别，还可以得到概率预测；模型可以作为baseline。<br>\n",
    "**缺点**：很难处理数据不平衡的问题，比如正负样本10000:1。\n",
    "1. CNN\n",
    "CNN是在做图像分类任务时使用到的，主要是基于AlexNet的预训练权重进行微调。<br>\n",
    "CNN局部感受野、权值共享的特性更适合图像；具体使用预训练的AlexNet，冻结前面卷积层，只微调后面的全连接层；小数据集下迁移学习比从零训练效果好。<br>\n",
    "1. RNN\n",
    "RNN是用来理解序列建模和上下文依赖的，RNN通过隐藏状态建模时间序列/文本序列，理论上可以捕捉上下文信息。<br>\n",
    "但长序列下会有梯度消失/梯度爆炸，RNN与前馈神经网络不同，后者不保留先前输入的信息，而RNN通过隐藏状态保持时间序列的上下文信息。<br>\n",
    "1. Transformer\n",
    "Transformer是在学习大模型时重点研究的架构，"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30dcfb9",
   "metadata": {},
   "source": [
    "**真实提问**\n",
    "1. 介绍一下你简历中提到的非线性transformer，跟常见的transformer有什么不同? "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
