{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc28a1b2",
   "metadata": {},
   "source": [
    "### 简历项目2：使用QLoRA+AWQ处理基于LLaMA3.1-8B的电商客服机器\n",
    "\n",
    "#### 1. 项目简介\n",
    "&emsp;&emsp; 在实际电商客服场景中，客服对话通常具有指令强、回复短、上下文依赖弱、风格固定等特点，而通用大模型往往存在回复冗长、不符合业务规范、推理成本高的问题。<br>\n",
    "&emsp;&emsp; 因此，本项目以电商客服单轮对话为切入点，利用真实业务数据对 LLaMA3.1-8B 进行 监督微调（SFT），使模型学习“客服式回答”的语言模式与约束。同时，通过 QloRA 技术在有限算力条件下完成高质量微调，并在此基础上进一步采用 AWQ 量化，在尽量不损失性能的前提下降低模型推理成本，从而探索一条 **小显存训练 + 低成本部署的电商大模型落地方案**。\n",
    "\n",
    "#### 2. 数据集\n",
    "&emsp;&emsp; **E_commerce_Customer_Service_dataset** 可以从魔搭社区下载，电商客户服务数据集，核心定位是为电商客服机器人的研究提供数据支持。它包含从lightinthebox网站收集的电商数据，包括通用问题的问答对以及商品信息，**样本展示**<br>\n",
    "&emsp;&emsp;{\"messages\": [{\"role\": \"system\", \"content\": \"你是一个专业的电商客服。严格遵守：1) 仅以‘assistant’角色回答一次；2) 不要提出新问题，不要延长对话；3) 不要生成‘user’或其他角色的内容；4) 输出只包含答案正文，无前后缀角色标签、无思考过程。\"}, {\"role\": \"user\", \"content\": \"那要怎样这也不行那也不行\"}, {\"role\": \"assistant\", \"content\": \"发邮政的哦\"}]} <br>\n",
    "&emsp;&emsp;{\"messages\": [{\"role\": \"system\", \"content\": \"你是一个专业的电商客服。严格遵守：1) 仅以‘assistant’角色回答一次；2) 不要提出新问题，不要延长对话；3) 不要生成‘user’或其他角色的内容；4) 输出只包含答案正文，无前后缀角色标签、无思考过程。\"}, {\"role\": \"user\", \"content\": \"可以优惠价么\"}, {\"role\": \"assistant\", \"content\": \"我们因为200把起给优惠价格您可以给您105元一把哦\"}]}<br>\n",
    "&emsp;&emsp;{\"messages\": [{\"role\": \"system\", \"content\": \"你是一个专业的电商客服。严格遵守：1) 仅以‘assistant’角色回答一次；2) 不要提出新问题，不要延长对话；3) 不要生成‘user’或其他角色的内容；4) 输出只包含答案正文，无前后缀角色标签、无思考过程。\"}, {\"role\": \"user\", \"content\": \"我去不早说发韵达能到我家那儿我就能拿到\"}, {\"role\": \"assistant\", \"content\": \"韵达不发的哦\"}]}<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a794c",
   "metadata": {},
   "source": [
    "#### 2. SFT阶段\n",
    "&emsp;&emsp; 引入 QloRA 方法，在冻结大部分参数的情况下完成高效微调，显著降低训练显存需求\n",
    "#### 3. AWQ量化阶段"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
